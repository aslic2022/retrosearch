[
  {
    "title": "Attention Is All You Need",
    "abstract": "This paper introduces the Transformer modelâ€”a novel architecture based solely on attention mechanisms that eliminates recurrence and convolutions. It demonstrates state-of-the-art performance on machine translation tasks while offering high parallelizability and reduced training time.",
    "link": "https://arxiv.org/pdf/1706.03762"
  },
  {
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "BERT presents a new language representation model that pre-trains deep bidirectional representations using masked language modeling and next sentence prediction. This approach enables state-of-the-art results on a wide range of NLP tasks with minimal task-specific architecture modifications.",
    "link": "https://arxiv.org/pdf/1810.04805"
  },
  {
    "title": "Deep contextualized word representations (ELMo)",
    "abstract": "ELMo introduces deep contextualized word representations that capture both the syntax and semantics of words as they vary across different contexts. These representations significantly improve performance on multiple NLP benchmarks by providing dynamic, context-sensitive embeddings.",
    "link": "https://arxiv.org/pdf/1802.05365"
  },
  {
    "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
    "abstract": "XLNet proposes a generalized autoregressive pretraining method that integrates ideas from autoregressive models and permutation language modeling. It effectively captures bidirectional context and outperforms BERT on various NLP benchmarks.",
    "link": "https://arxiv.org/pdf/1906.08237"
  },
  {
    "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
    "abstract": "ALBERT improves upon BERT by significantly reducing the number of parameters through factorized embedding parameterization and cross-layer parameter sharing, while still achieving competitive performance on a broad range of NLP tasks.",
    "link": "https://arxiv.org/pdf/1909.11942"
  }
]
